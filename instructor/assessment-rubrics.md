# CSC-113 Assessment Rubrics & Grading Framework
## "Grading AI Collaboration, Not AI Avoidance"

**Course Philosophy:** This course requires AI use for all assignments. Assessment focuses on effective AI partnership, critical thinking, and professional workflow rather than penalizing AI assistance.

---

## ðŸ“‹ Table of Contents

1. [Core Assessment Philosophy](#core-assessment-philosophy)
2. [Four-Category Framework](#four-category-framework)
3. [Competency Levels](#competency-levels)
4. [Assignment-Specific Rubrics](#assignment-specific-rubrics)
5. [Observable Behaviors Guide](#observable-behaviors-guide)
6. [Peer Review Rubrics](#peer-review-rubrics)
7. [Portfolio Assessment](#portfolio-assessment)

---

## Core Assessment Philosophy

### What We're Actually Measuring

**âœ… DO Grade:**
- AI collaboration effectiveness
- Critical evaluation of AI outputs
- Process documentation quality
- Professional workflow consistency
- Iterative improvement evidence
- Problem-solving approach
- Honest reflection and growth

**âŒ DON'T Grade:**
- "Original" work without AI assistance
- Perfect technical implementation
- AI-free knowledge demonstration
- Comparison to pre-AI academic standards
- Quantity of code or complexity

### The "Checkmate" Principle

> "Course requires AI use for all assignments. This flips the academic integrity problem by making AI partnership mandatory and grading on collaboration quality."

**Key Questions:**
- How effectively does the student work with AI tools?
- Can they critically evaluate AI outputs?
- Do they understand what the AI did and why?
- Can they explain trade-offs and limitations?

---

## Four-Category Framework

Every assignment is assessed across four categories (25% each = 6.25 points):

### Category 1: AI Partnership Quality (25%)

**What This Measures:**
- How effectively the student collaborates with AI tools
- Prompt sophistication and iteration
- Appropriate tool selection for tasks
- Evidence of learning from AI interactions
- Critical evaluation of AI outputs

**Observable Evidence:**
- Prompt iterations documented (3+ versions)
- Reasoning for tool selection
- Testing multiple approaches
- Reflections showing evaluation ("This worked because...")
- Documentation of AI collaboration process

**Scoring:**

**Excellent (5.5-6.25 points):**
- Systematically tested multiple AI tools or approaches
- Documented clear iteration and refinement process
- Showed informed tool selection with specific reasoning
- Evidence of critical evaluation, not blind acceptance
- Sophisticated prompt engineering techniques

**Good (4.5-5.4 points):**
- Tested required tools with some comparison
- Evidence of at least 2 prompt iterations
- Tool selection with basic reasoning provided
- Some critical evaluation evident
- Basic to intermediate prompt techniques

**Needs Improvement (<4.5 points):**
- Testing incomplete or superficial
- No clear evidence of iteration
- Random or unexplained tool selection
- Accepts AI output uncritically
- Generic or copy-paste prompts only

### Category 2: Problem-Solving Process (25%)

**What This Measures:**
- Systematic approach to challenges
- Clear problem definition
- Logical solution development
- Testing methodology
- Learning from failures
- Process documentation

**Observable Evidence:**
- Problem clearly defined before solving
- Systematic testing approach documented
- Evidence of iteration and refinement
- Failures documented and analyzed
- Logical progression visible in commits

**Scoring:**

**Excellent (5.5-6.25 points):**
- Followed all workflow steps systematically
- Well-organized workspace and file structure
- Clear documentation of approach
- Comprehensive testing with results
- Failures analyzed for learning
- Reflections show metacognitive awareness

**Good (4.5-5.4 points):**
- Followed most workflow steps
- Generally organized workspace
- Basic documentation of approach
- Some testing evidence
- Mentions challenges encountered
- Some reflection on process

**Needs Improvement (<4.5 points):**
- Skipped key workflow steps
- Disorganized or unclear structure
- Minimal process documentation
- No clear testing methodology
- Failures ignored or hidden
- Superficial or missing reflections

### Category 3: Professional Communication (25%)

**What This Measures:**
- Documentation clarity and completeness
- GitHub portfolio quality
- Peer review participation and quality
- Professional presentation standards
- Commit messages and PR descriptions

**Observable Evidence:**
- All required files present and well-formatted
- Clear, professional writing style
- Helpful peer review comments
- Good commit messages
- Professional README and portfolio

**Scoring:**

**Excellent (5.5-6.25 points):**
- All required files present, professional format
- Documentation others can understand and use
- Thoughtful, constructive peer reviews
- Excellent commit messages and PR descriptions
- Portfolio demonstrates professional standards
- Appropriate communication for different audiences

**Good (4.5-5.4 points):**
- Most required files present
- Generally clear documentation
- Adequate peer reviews provided
- Decent commit messages
- Portfolio shows effort and organization
- Communication mostly appropriate

**Needs Improvement (<4.5 points):**
- Missing required files
- Unclear or unprofessional writing
- Poor or missing peer reviews
- Generic commit messages ("fix", "update")
- Disorganized portfolio
- Inappropriate or unclear communication

### Category 4: Critical Thinking & Ethics (25%)

**What This Measures:**
- Questioning AI outputs rather than accepting blindly
- Identifying bias, limitations, ethical implications
- Making informed decisions about AI use
- Understanding broader implications
- Demonstrating growth mindset

**Observable Evidence:**
- Questions and challenges AI suggestions
- Identifies potential problems or limitations
- Makes thoughtful decisions about when to override AI
- Considers ethical implications
- Shows honest assessment of challenges
- Demonstrates learning and growth

**Scoring:**

**Excellent (5.5-6.25 points):**
- Consistently questions and evaluates AI outputs
- Identifies specific biases or limitations
- Makes well-reasoned decisions about AI advice
- Thoughtful consideration of ethical implications
- Honest, insightful reflections
- Clear evidence of growth mindset

**Good (4.5-5.4 points):**
- Some questioning of AI outputs
- Mentions rate limits or practical constraints
- Basic reasoning about AI use decisions
- Some ethical consideration
- Identifies some challenges faced
- Shows awareness of learning process

**Needs Improvement (<4.5 points):**
- Uncritical acceptance of all AI outputs
- No consideration of limitations or bias
- Random decisions about tool use
- No ethical reasoning evident
- Generic or defensive reflections
- Fixed mindset or resistance to feedback

---

## Competency Levels

### Progression Framework

**Novice:**
- Basic prompt use without iteration
- Accepts AI output uncritically
- Minimal documentation
- Struggles with workflow
- Generic reflections

**Developing:**
- Some prompt iteration (2 versions)
- Beginning to evaluate outputs
- Basic documentation present
- Following workflow with guidance
- Identifies some challenges

**Proficient:**
- Systematic prompt refinement (3+ versions)
- Critical evaluation evident
- Clear, organized documentation
- Consistent professional workflow
- Thoughtful reflection with insights

**Advanced:**
- Sophisticated AI orchestration
- Meta-cognitive awareness
- Excellent documentation practices
- Professional-quality workflow
- Deep insights and transfer learning

### Observable Behaviors by Level

**Prompt Engineering:**
- Novice: "help me with [task]"
- Developing: "act as [role] and help me [task] focusing on [aspect]"
- Proficient: Context + constraints + examples + desired format
- Advanced: Systematic testing of prompt variations with analysis

**Documentation:**
- Novice: Minimal or missing
- Developing: Present but unclear
- Proficient: Clear and complete
- Advanced: Exceptional clarity and insight

**Reflection:**
- Novice: "It was hard/good/fine"
- Developing: "I learned [thing]. It was challenging when [situation]"
- Proficient: "I discovered [insight] through [process]. Next time I'd [specific change]"
- Advanced: Transfer learning, meta-cognition, connects to bigger ideas

---

## Assignment-Specific Rubrics

### Week 3-4: Bad Bot / Good Bot Sequence

**Total Points: 25 (per assignment)**

#### Bad Bot Assignment (Week 3)

**Problem Identification (25% = 6.25 pts)**

**Excellent (5.5-6.25):**
- Identifies ONE specific, measurable friction point
- Provides clear evidence why this is the priority
- Shows understanding of UX principles
- Documents actual user pain, not theoretical issues

**Good (4.5-5.4):**
- Identifies real problem clearly
- Some reasoning for priority
- Basic UX understanding
- Has evidence of problem

**Needs Work (<4.5):**
- No clear problem OR multiple problems bundled
- Limited reasoning
- Confused about user vs. developer problems
- Weak or missing evidence

**Test Documentation (25% = 6.25 pts)**

**Excellent (5.5-6.25):**
- 3+ concrete test cases with actual results
- Clear "Expected vs. Got" format
- Evidence of systematic testing
- Screenshots or specific examples

**Good (4.5-5.4):**
- 2-3 test cases documented
- Basic testing approach
- Some concrete examples
- Generally clear results

**Needs Work (<4.5):**
- < 2 test cases OR vague testing
- No clear methodology
- Generic descriptions
- Missing actual results

**Bot Character Preservation (25% = 6.25 pts)**

**Excellent (5.5-6.25):**
- Bot has clear, distinctive personality
- Friction is intentional and specific
- Character consistent throughout
- Creative and original

**Good (4.5-5.4):**
- Bot has personality
- Friction present
- Mostly consistent
- Follows assignment guidelines

**Needs Work (<4.5):**
- Generic bot personality
- Unclear or random friction
- Inconsistent character
- Doesn't follow guidelines

**Reflection Quality (25% = 6.25 pts)**

**Excellent (5.5-6.25):**
- Honest assessment of what's annoying/funny
- Connects to UX principles
- Predicts improvement opportunities
- Shows insight and humor

**Good (4.5-5.4):**
- Describes bot behavior
- Notes friction points
- Some analysis of why
- Basic reflection

**Needs Work (<4.5):**
- Generic statements
- No real analysis
- Defensive about bot
- Missing connection to learning

#### Good Bot Assignment (Week 4)

**Change Quality (25% = 6.25 pts)**

**Excellent (5.5-6.25):**
- Makes EXACTLY one surgical change
- Change directly addresses identified problem
- Preserves bot personality completely
- Shows creative problem-solving within constraints

**Good (4.5-5.4):**
- One clear change made
- Change relates to problem
- Mostly preserves personality
- Follows the rules

**Needs Work (<4.5):**
- Multiple changes OR complete personality removal
- Change doesn't address stated problem
- No change made ("it's perfect")

**Trade-Off Analysis (25% = 6.25 pts)**

**Excellent (5.5-6.25):**
- Identifies specific trade-offs made
- Analyzes what was gained vs. lost
- Shows understanding of design decisions
- Honest about remaining friction

**Good (4.5-5.4):**
- Mentions trade-offs
- Basic analysis present
- Some awareness of compromises
- Notes what changed

**Needs Work (<4.5):**
- Claims no trade-offs exist
- No analysis of decisions
- Defensive about changes
- Missing understanding of constraints

**Testing Evidence (25% = 6.25 pts)**

**Excellent (5.5-6.25):**
- Same test cases re-run with new results
- Clear before/after comparison
- Quantifiable improvements shown
- Evidence of systematic verification

**Good (4.5-5.4):**
- Test cases repeated
- Some comparison shown
- Basic verification
- Results documented

**Needs Work (<4.5):**
- Tests not repeated
- No clear comparison
- Claims of improvement without evidence
- Missing verification

**Lesson Learned (25% = 6.25 pts)**

**Excellent (5.5-6.25):**
- Specific insights about incremental improvement
- Connects to larger design principles
- Shows transfer learning potential
- Honest about challenges and learnings

**Good (4.5-5.4):**
- States something learned
- Makes connection to assignment goals
- Some insight present
- Acknowledges challenges

**Needs Work (<4.5):**
- Generic "I learned a lot"
- No specific insights
- Misses assignment purpose
- Superficial reflection

---

## Observable Behaviors Guide

### What Excellent Work Looks Like

**In Prompts:**
```
âŒ Novice: "help me code a calculator"

âœ… Proficient: "Act as a Python tutor. Help me create a basic calculator 
that handles +, -, *, /. Use functions for each operation. Include error 
handling for division by zero. Explain each part as you go."

âœ… Advanced: [Same as proficient, PLUS] "First, show me three different 
approaches to organizing this code. Then we'll pick one and iterate on it."
```

**In Documentation:**
```
âŒ Novice: "I made a calculator. It works."

âœ… Proficient: "## Calculator Implementation

I tested three approaches:
1. Single function with if/elif (too messy)
2. Separate functions (chosen - clearer)
3. Class-based (overkill for this task)

Chose #2 because [specific reasoning].

The AI initially suggested [X] but I modified it to [Y] because [reason]."

âœ… Advanced: [Same as proficient, PLUS analysis of trade-offs, 
discussion of when different approaches would be better, connection to 
software design principles]
```

**In Reflection:**
```
âŒ Novice: "This assignment was hard but I learned about AI."

âœ… Proficient: "I discovered that more specific prompts give better results. 
My first attempt failed because I didn't specify the output format. After 
adding format requirements, the AI generated code I could actually use."

âœ… Advanced: [Same as proficient, PLUS] "This taught me that human judgment 
is crucial in AI partnership - I need to evaluate outputs, not just accept 
them. Next project, I'll start by defining success criteria before prompting."
```

### Red Flags in Student Work

**Indicates Needs Improvement:**
- Empty or generic commit messages ("update", "fix stuff")
- No evidence of iteration or refinement
- Claims perfection or no problems encountered
- Defensive language about choices made
- Missing required documentation
- Copied prompts without customization
- No testing methodology visible
- Generic reflections without specific examples

**Indicates Possible Academic Integrity Issue:**
- Work dramatically exceeds demonstrated skill level
- No process documentation despite complex output
- Can't explain their own work in follow-up
- Identical to another student's work
- No evidence of AI collaboration (suspiciously AI-free)

---

## Peer Review Rubrics

### Grading Student Reviews

**Peer reviews are worth 10% of assignment grade**

**Excellent Peer Review (9-10 points):**
- Specific, actionable feedback with examples
- Identifies both strengths and improvement areas
- Asks thoughtful questions
- Respectful and constructive tone
- References assignment rubric
- Suggests concrete next steps

**Good Peer Review (7-8 points):**
- Generally helpful feedback
- Notes some specific points
- Mostly constructive
- Shows engagement with work
- Basic suggestions provided

**Needs Improvement (<7 points):**
- Generic comments only ("looks good", "nice work")
- Harsh or unconstructive criticism
- Barely engages with submission
- Missing or superficial
- Doesn't reference assignment goals

### Peer Review Training (Week 2)

**Teach Students to Ask:**
1. "What problem were they trying to solve?"
2. "Does their solution address that problem?"
3. "What trade-offs did they make?"
4. "What's one specific thing they could improve?"
5. "What's one thing they did really well?"

**Provide Comment Templates:**

```markdown
**Strengths I noticed:**
- [Specific example]
- [Specific example]

**Questions I have:**
- [Genuine curiosity about choices made]

**Suggestions for improvement:**
- [Specific, actionable item with reasoning]

**What I learned from your work:**
- [Transfer learning / insight gained]
```

---

## Portfolio Assessment

### End-of-Semester Portfolio Review

**Total Portfolio Grade: 100 points**

**Portfolio Quality (30 points):**
- Professional README introduction
- Clear project organization
- All assignments linked and accessible
- Consistent commit history
- Evidence of growth over time

**AI Collaboration Demonstrated (30 points):**
- Progression from basic to sophisticated prompting
- Evidence of critical evaluation throughout
- Documentation of AI partnership process
- Honest reflection on what worked/didn't

**Professional Workflow (20 points):**
- Proper GitHub usage (Issues, PRs, commits)
- Documentation standards maintained
- Peer review participation
- Communication quality

**Growth & Learning (20 points):**
- Visible improvement over semester
- Thoughtful reflections showing insight
- Evidence of transfer learning
- Capstone demonstrates integration

### Portfolio Review Checklist

```markdown
â–¡ README tells clear story of learning journey
â–¡ All major assignments present and linked
â–¡ Commit history shows consistent work
â–¡ Documentation is clear and professional
â–¡ Evidence of AI tool usage progression
â–¡ Critical thinking visible in reflections
â–¡ Professional communication throughout
â–¡ Capstone project demonstrates competency
â–¡ Growth from Week 1 to Week 16 is clear
â–¡ Student can explain their work and choices
```

---

## Grading Philosophy Summary

### Remember

> "A perfect score doesn't mean a perfect product. It means: perfect understanding of the assignment, perfect execution of the process, perfect documentation of the journey, perfect honesty about the results."

### The "Slightly Less Terrible" Standard

From Bad Bot/Good Bot sequence:
- Best submissions often have the funniest remaining flaws
- Perfect bots miss the point
- Learning happens in iteration and constraint
- Process > Product

### Academic Honesty in AI-Required Work

**Red flags that need investigation:**
- No evidence of AI use (suspiciously AI-free)
- Can't explain their own documented process
- Work quality dramatically inconsistent
- Missing all process documentation

**NOT academic dishonesty:**
- Using AI extensively (that's required!)
- Getting help from AI on everything
- Simple solutions that work
- Borrowing ideas from examples (if cited)

---

## Quick Reference Card

### Four Categories Quick Check

**AI Partnership:** Did they iterate prompts? Evaluate outputs critically?  
**Problem-Solving:** Clear approach? Testing? Learning from failures?  
**Communication:** Documentation clear? Professional? Helpful reviews?  
**Critical Thinking:** Question AI? Consider ethics? Show growth?

### Scoring Guidelines

- **6.25 pts:** Exceptional work demonstrating mastery
- **5.5 pts:** Very good work, minor improvements possible
- **5.0 pts:** Solid proficient work
- **4.5 pts:** Good but needs strengthening
- **4.0 pts:** Developing, significant room for growth
- **<4.0 pts:** Needs major improvement or missing requirements

### Common Grading Questions

**Q: "Their code doesn't work but process is good. What grade?"**  
A: Focus on process! If they documented what they tried, why it didn't work, and what they learned, that's often more valuable than working code.

**Q: "They made the bot too helpful. Did they miss the point?"**  
A: For Bad Bot, yes. For Good Bot, maybe not - did they make ONE change that reduced friction while keeping personality? If so, that's the assignment.

**Q: "Their reflection is short. Does that matter?"**  
A: Quality > quantity. A short, insightful reflection beats a long generic one. But it should demonstrate actual thought.

**Q: "They clearly used AI for everything. Is that okay?"**  
A: YES! That's required! Grade on whether they used it WELL and critically evaluated the output.

---

*Version 1.0 - November 4, 2025*  
*Related Documents: operations-manual.md, example-submissions/, peer-review-guide.md*

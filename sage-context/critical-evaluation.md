# Critical Evaluation - SAGE Context Document

## Purpose

This document helps SAGE teach students how to critically evaluate AI outputs. Students will ask questions like "Is this answer accurate?" and "How do I know when to trust AI?" and SAGE should guide them toward developing calibrated judgment and healthy skepticism.

---

## Core Concepts

### Why Critical Evaluation Matters

AI tools are powerful but imperfect. They can:
- Generate confident-sounding but incorrect information ("hallucinations")
- Miss important context or nuance
- Reflect biases in their training data
- Provide outdated information
- Excel at some tasks while failing at others

**Your job as an AI user is not to blindly trust or reject AI outputs—it's to develop calibrated judgment about when and how to verify.**

The goal isn't paranoia. It's appropriate confidence: knowing when you can rely on AI and when you need to double-check.

### The Verification Mindset

Think of AI outputs like advice from a very knowledgeable friend who sometimes makes things up:

1. **Consider the source** - AI is good at some things, bad at others
2. **Check the stakes** - Low-stakes tasks need less verification
3. **Look for red flags** - Certain patterns suggest problems
4. **Verify when it matters** - Not everything needs fact-checking
5. **Trust but verify** - Use AI as a starting point, not final answer

---

## Signs of AI Hallucination

### Red Flags to Watch For

**Overly Specific Details**
When AI provides very specific numbers, dates, quotes, or statistics—especially for obscure topics—be suspicious. Real specificity requires real sources.

Examples:
- "The study by Dr. Johnson in 2019 found that 73.4% of participants..."
- "According to the April 2022 report by the Association of..."

**These might be real, but verify them.** AI can fabricate citations that sound authentic.

**Confident Tone on Uncertain Topics**
AI rarely says "I don't know." Watch for confident answers on topics where uncertainty would be appropriate:
- Predictions about the future
- Personal opinions disguised as facts
- Emerging or rapidly changing fields
- Niche or specialized knowledge

**Internal Contradictions**
If an AI's response contradicts itself or earlier responses in the same conversation, something's wrong. Ask for clarification.

**Logical Inconsistencies**
Conclusions that don't follow from premises, or advice that contradicts common sense, warrant scrutiny.

**Suspiciously Perfect Answers**
If an answer perfectly matches what you wanted to hear, pause. AI is trained to be helpful, which can mean telling you what you want rather than what's true.

### What AI Does Well vs. Poorly

| Task Type | AI Strength | Verification Need |
|-----------|-------------|-------------------|
| **Explaining concepts** | Strong | Low - but check for oversimplification |
| **Brainstorming** | Strong | Low - ideas are starting points |
| **Code syntax** | Strong | Medium - test the code |
| **Current events** | Weak | High - knowledge cutoff issues |
| **Specific facts/stats** | Medium | High - verify sources |
| **Personal advice** | Medium | Medium - consider context |
| **Creative writing** | Strong | Low - subjective |
| **Math calculations** | Variable | High - verify important calculations |
| **Legal/medical info** | Weak | Very High - always consult professionals |

---

## How to Verify AI Claims

### The Verification Ladder

**Level 1: Quick Sanity Check (5 seconds)**
- Does this make basic sense?
- Does it contradict something I know is true?
- Would this be obviously wrong to an expert?

**Level 2: Cross-Reference (1-2 minutes)**
- Ask the same question to a different AI
- Do a quick web search for the main claim
- Check if the cited sources exist

**Level 3: Source Verification (5-15 minutes)**
- Find and read the original source
- Check the author's credentials
- Look for corroborating sources
- Check publication date

**Level 4: Expert Consultation (varies)**
- Ask someone knowledgeable
- Consult official documentation
- Use authoritative databases

### When to Use Each Level

| Stakes | Verification Level |
|--------|---------------------|
| **Just curious** | Level 1 |
| **Homework/learning** | Level 2 |
| **Work deliverables** | Level 2-3 |
| **Important decisions** | Level 3-4 |
| **Legal/medical/financial** | Level 4 + professional |

### Verification Techniques

**Ask for Sources**
Tell the AI: "Please provide sources for these claims."

Then CHECK if those sources:
- Actually exist
- Say what the AI claims they say
- Are credible

**Triangulate**
Get the same information from 3 different places. If they agree, more likely to be accurate. If they disagree, investigate further.

**Ask Follow-Up Questions**
Probe for details:
- "How do you know this?"
- "What's the evidence for this claim?"
- "Are there any competing views on this?"

If the AI struggles to provide coherent follow-ups, the original answer may be shaky.

**Test in Practice**
For code, run it. For processes, try them. For advice, consider a small test. Reality is the ultimate fact-checker.

---

## Building Calibrated Confidence

### What is Calibration?

Calibration means your confidence matches your accuracy. If you're 80% confident about something, you should be right about 80% of the time.

**Over-confident**: Trusting AI too much, not verifying important claims
**Under-confident**: Wasting time verifying everything, not using AI effectively
**Well-calibrated**: Knowing when to trust and when to verify

### Developing Your Calibration

**Keep Track of Errors**
When AI gives you wrong information, note it:
- What type of question was it?
- What red flags did you miss?
- How could you have caught it?

Over time, you'll develop intuition for when to be skeptical.

**Notice Your Assumptions**
When you assume AI is right, ask yourself why. Is it because:
- AI is generally reliable for this type of question?
- You haven't thought to question it?
- Verification would be inconvenient?

**Practice Deliberate Verification**
Even when you think AI is right, occasionally verify anyway. This builds your sense of AI accuracy and catches errors you'd otherwise miss.

### The Cost-Benefit of Verification

Verification takes time. Consider:
- **How long to verify?** (Quick search vs. deep research)
- **What if AI is wrong?** (Minor inconvenience vs. major problem)
- **How likely is error?** (Common knowledge vs. obscure facts)

**Rule of thumb:** Invest verification time proportional to the cost of being wrong.

---

## Common Questions

**Q: Should I fact-check everything AI tells me?**
**A:** No. That would defeat the purpose of using AI. Develop calibration: verify when stakes are high, when you spot red flags, or when something seems off. For low-stakes learning and brainstorming, reasonable trust is efficient.

**Q: Is AI just lying to me?**
**A:** AI doesn't "lie" in the intentional sense. It generates plausible-sounding text based on patterns. Sometimes those patterns lead to accurate information, sometimes to confident-sounding nonsense. Understanding this helps you evaluate appropriately.

**Q: How do I know if a source AI cites is real?**
**A:** Search for the exact title, author, and publication. If you can't find it, it may be fabricated. Even if you find it, verify it says what AI claims it says.

**Q: What if different AIs give different answers?**
**A:** Disagreement suggests uncertainty. Investigate further, or consider that there may be multiple valid perspectives. Agreement across multiple sources increases (but doesn't guarantee) reliability.

**Q: Can I train myself to spot AI errors?**
**A:** Yes! With practice, you develop intuition for AI failure modes. The more you deliberately verify and learn from errors, the better your judgment becomes.

---

## Evaluation Framework

When you receive an AI output, run through this quick checklist:

### Initial Assessment
- [ ] Does this make basic sense?
- [ ] Does it match what I already know?
- [ ] Are there any obvious red flags?

### Confidence Calibration
- [ ] What type of task is this? (AI strength or weakness?)
- [ ] How specific are the claims?
- [ ] How confident is the AI's tone?

### Stakes Assessment
- [ ] What happens if this is wrong?
- [ ] Is this for learning, work, or important decisions?
- [ ] Does this affect other people?

### Verification Decision
- [ ] Low stakes + no red flags = Reasonable to trust
- [ ] Medium stakes OR red flags = Quick verification
- [ ] High stakes = Thorough verification
- [ ] Very high stakes = Expert consultation

---

## Common AI Failure Modes

### Mode 1: Confident Fabrication
AI states false information with complete confidence. Often includes fake citations or statistics.

**Prevention:** Verify specific claims, especially citations.

### Mode 2: Plausible Nonsense
Response sounds reasonable but is subtly wrong. Often occurs with technical or specialized topics.

**Prevention:** Test in practice; consult domain experts.

### Mode 3: Outdated Information
AI's knowledge has a cutoff date. Information about recent events may be wrong or missing.

**Prevention:** Check dates; verify current information independently.

### Mode 4: Context Confusion
AI misinterprets your question or loses track of context in long conversations.

**Prevention:** Be explicit about context; verify AI understood your question.

### Mode 5: Over-Simplification
AI provides a correct but incomplete answer that misses important nuances.

**Prevention:** Ask follow-up questions; consider complexity.

### Mode 6: Inconsistent Reasoning
AI contradicts itself or applies logic inconsistently.

**Prevention:** Check for internal consistency; probe with follow-ups.

---

## What SAGE Should NOT Do

- Present AI-generated information as automatically true
- Discourage students from verifying important claims
- Make students feel paranoid about all AI output
- Skip the nuance between blind trust and excessive skepticism
- Provide false certainty about AI accuracy

**When to escalate:** If a student is making important decisions based on unverified AI information, remind them to verify. If the stakes are high (legal, medical, financial), always recommend professional consultation.

---

## Connections

This document connects to:

- **SAGE-core.md** - SAGE's honest approach to its own limitations
- **prompt-engineering.md** - How to get better outputs (reducing need for correction)
- **debugging-guide.md** - Systematic problem-solving when AI gives bad information
- **course-overview.md** - Critical thinking as a core course competency

---

## Remember

The goal isn't to distrust AI—it's to trust it appropriately.

Good AI users develop calibrated confidence:
- They leverage AI's strengths without being naive
- They verify when stakes are high or red flags appear
- They learn from errors to improve their judgment
- They use AI as a powerful tool, not an infallible oracle

**Critical evaluation isn't about being paranoid. It's about being smart.**

Every professional who uses AI tools learns this skill. You're developing it now, which will serve you throughout your career.

> "Trust, but verify." - Classic wisdom, perfectly applicable to AI.
